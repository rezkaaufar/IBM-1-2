{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuno-mota/Programas/Anaconda/Installation/lib/python2.7/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import itertools as it\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.word_tokenize\n",
    "import collections\n",
    "import operator\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "def get_types_list(formatted_language_corpus):\n",
    "    language_types_list = []\n",
    "    for sentence in formatted_language_corpus:\n",
    "        for word in sentence:\n",
    "            if (word not in language_types_list):\n",
    "                language_types_list.append(word)\n",
    "                \n",
    "    return language_types_list\n",
    "\n",
    "def get_types_list(formatted_language_corpus):\n",
    "    language_types_list = []\n",
    "    for sentence in formatted_language_corpus:\n",
    "        for word in sentence:\n",
    "            if (word not in language_types_list):\n",
    "                language_types_list.append(word)\n",
    "                \n",
    "    return language_types_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def escape(str):\n",
    "    str = str.replace(\"&lt;\", \"\")\n",
    "    str = str.replace(\"lt;\", \"\")\n",
    "    str = str.replace(\"&gt;\", \"\")\n",
    "    str = str.replace(\"gt;\", \"\")\n",
    "    str = str.replace(\"&amp;\", \"\")\n",
    "    str = str.replace(\"amp;\", \"\")\n",
    "    str = str.replace(\"&quot;\", \"\")\n",
    "    str = str.replace(\"quot;\", \"\")\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eng_ind = collections.defaultdict(list)\n",
    "ger_ind = collections.defaultdict(list)\n",
    "fre_ind = collections.defaultdict(list)\n",
    "\n",
    "start = '<seg id=\"'\n",
    "end = '\" type'\n",
    "whitespace = \"\\r\\n\\t\"\n",
    "\n",
    "high_sentence1 = \"English.xml\"\n",
    "high_sentence2 = \"German.xml\"\n",
    "high_sentence3 = \"French.xml\"\n",
    "low_sentence   = \"Spanish.xml\"\n",
    "\n",
    "# Read the segments of the English, German, and French bible\n",
    "with open(high_sentence1) as f:\n",
    "    line = f.readlines()\n",
    "    for i in range(len(line)):\n",
    "        if \"seg id=\" in line[i] and \"seg id=\" not in line[i+1]:\n",
    "            res = escape(line[i+1])\n",
    "            eng_ind[line[i][line[i].find(start)+len(start):line[i].find(end)]].append(res.strip(whitespace))\n",
    "            \n",
    "with open(high_sentence2) as f:\n",
    "    line = f.readlines()\n",
    "    for i in range(len(line)):\n",
    "        if \"seg id=\" in line[i] and \"seg id=\" not in line[i+1]: \n",
    "            res = escape(line[i+1])\n",
    "            ger_ind[line[i][line[i].find(start)+len(start):line[i].find(end)]].append(res.strip(whitespace))\n",
    "\n",
    "with open(high_sentence3) as f:\n",
    "    line = f.readlines()\n",
    "    for i in range(len(line)):\n",
    "        if \"seg id=\" in line[i] and \"seg id=\" not in line[i+1]:\n",
    "            res = escape(line[i+1])\n",
    "            fre_ind[line[i][line[i].find(start)+len(start):line[i].find(end)]].append(res.strip(whitespace))            \n",
    "\n",
    "# read the segments of low resource sentence\n",
    "with open(low_sentence) as f:\n",
    "    line = f.readlines()\n",
    "    for i in range(len(line)):\n",
    "        if \"seg id=\" in line[i] and \"seg id=\" not in line[i+1]:\n",
    "            res = escape(line[i+1])\n",
    "            eng_ind[line[i][line[i].find(start)+len(start):line[i].find(end)]].append(res.strip(whitespace))\n",
    "            ger_ind[line[i][line[i].find(start)+len(start):line[i].find(end)]].append(res.strip(whitespace))\n",
    "            fre_ind[line[i][line[i].find(start)+len(start):line[i].find(end)]].append(res.strip(whitespace))\n",
    "\n",
    "# remove sentence that does not have a translation\n",
    "keys_to_remove = [key for key, value in eng_ind.iteritems() if len(eng_ind[key]) == 1]\n",
    "keys_to_remove2 = [key for key, value in ger_ind.iteritems() if len(ger_ind[key]) == 1]\n",
    "keys_to_remove3 = [key for key, value in fre_ind.iteritems() if len(fre_ind[key]) == 1]\n",
    "\n",
    "for key in keys_to_remove:\n",
    "    del eng_ind[key]\n",
    "for key in keys_to_remove2:\n",
    "    del ger_ind[key]\n",
    "for key in keys_to_remove3:\n",
    "    del fre_ind[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set = None\n",
    "training_set_2 = None\n",
    "training_set_3 = None\n",
    "test_set = None\n",
    "test_set_2 = None\n",
    "test_set_3 = None\n",
    "keys_test_set = None\n",
    "\n",
    "\n",
    "def pick_test_sentences(number_of_test_sentences):\n",
    "    \n",
    "    global test_set\n",
    "    global test_set_2\n",
    "    global test_set_3\n",
    "    global keys_test_set\n",
    "    \n",
    "    test_set = []\n",
    "    test_set_2 = []\n",
    "    test_set_3 = []\n",
    "    \n",
    "    keys_test_set = []\n",
    "    while(len(keys_test_set) != number_of_test_sentences):\n",
    "        temp = random.choice(eng_ind.keys(), number_of_test_sentences - len(keys_test_set))\n",
    "        for key in temp:\n",
    "            if(key in eng_ind.keys() and key in ger_ind.keys() and key in fre_ind.keys() and\n",
    "               key not in keys_test_set):\n",
    "                keys_test_set.append(key)\n",
    "                \n",
    "    for key in keys_test_set:\n",
    "        test_set.append(eng_ind[key])\n",
    "        test_set_2.append(ger_ind[key])\n",
    "        test_set_3.append(fre_ind[key])\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "def pick_training_sentences(number_of_training_sentences):\n",
    "    \n",
    "    global training_set\n",
    "    global training_set_2\n",
    "    global training_set_3\n",
    "    global keys_test_set\n",
    "    \n",
    "    \n",
    "    training_set = []\n",
    "    training_set_2 = [] \n",
    "    training_set_3 = []\n",
    "\n",
    "\n",
    "    keys_training_set = []\n",
    "    while(len(keys_training_set) != number_of_training_sentences):\n",
    "        temp = random.choice(eng_ind.keys(), number_of_training_sentences - len(keys_training_set))\n",
    "        for key in temp:\n",
    "            if(key in eng_ind.keys() and key in ger_ind.keys() and key in fre_ind.keys() and\n",
    "               key not in keys_test_set and key not in keys_training_set):\n",
    "                keys_training_set.append(key)\n",
    "\n",
    "        \n",
    "    for key in keys_training_set:\n",
    "        training_set.append(eng_ind[key])\n",
    "        training_set_2.append(ger_ind[key])\n",
    "        training_set_3.append(fre_ind[key])\n",
    "            \n",
    "    \n",
    "    return None\n",
    "            \n",
    "#pick_training_sentences(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_training_set(number):\n",
    "    global training_set\n",
    "    global training_set_2\n",
    "    global training_set_3\n",
    "    with open(\"experiment_data/english_\" + str(number) + \".txt\", 'wb') as f:\n",
    "        pickle.dump(training_set, f)\n",
    "    with open(\"experiment_data/german_\" + str(number) + \".txt\", 'wb') as f:\n",
    "        pickle.dump(training_set_2, f)\n",
    "    with open(\"experiment_data/french_\" + str(number) + \".txt\", 'wb') as f:\n",
    "        pickle.dump(training_set_3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_test_set():\n",
    "    global test_set\n",
    "    global test_set_2\n",
    "    global test_set_3\n",
    "    with open(\"experiment_data/english_test.txt\", 'wb') as f:\n",
    "        pickle.dump(test_set, f)\n",
    "    with open(\"experiment_data/german_test.txt\", 'wb') as f:\n",
    "        pickle.dump(test_set_2, f)\n",
    "    with open(\"experiment_data/french_test.txt\", 'wb') as f:\n",
    "        pickle.dump(test_set_3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dump_model(pt_filename, pt):\n",
    "    with open(\"experiment_data/model/\" + pt_filename, 'wb') as handle:\n",
    "        pickle.dump(pt, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_token_train = None\n",
    "spanish_token_train = None\n",
    "german_token_train = None\n",
    "spanish_token_train_2 = None\n",
    "french_token_train = None\n",
    "spanish_token_train_3 = None\n",
    "english_token_test = None\n",
    "spanish_token_test = None\n",
    "german_token_test = None\n",
    "spanish_token_test_2 = None\n",
    "french_token_test = None\n",
    "spanish_token_test_3 = None\n",
    "\n",
    "language_train_pairs = None\n",
    "language_test_pairs = None\n",
    "language_train_pairs_2 = None\n",
    "language_test_pairs_2 = None\n",
    "language_train_pairs_3 = None\n",
    "language_test_pairs_3 = None\n",
    "\n",
    "\n",
    "def tokenize_test():\n",
    "\n",
    "    global english_token_test\n",
    "    global german_token_test\n",
    "    global french_token_test\n",
    "    global spanish_token_test\n",
    "    global language_test_pairs\n",
    "    global language_test_pairs_2\n",
    "    global language_test_pairs_3\n",
    "    \n",
    "    english_token_test = []\n",
    "    german_token_test = []\n",
    "    french_token_test = []\n",
    "    spanish_token_test = []\n",
    "    spanish_token_test_2 = []\n",
    "    spanish_token_test_3 = []\n",
    "    \n",
    "\n",
    "    for test in test_set:\n",
    "        temp = \"null \" + test[0]\n",
    "        temp2 = test[1] + \".\"\n",
    "        english_token_test.append(tokenizer(temp.lower().decode('utf-8')))\n",
    "        spanish_token_test.append(tokenizer(temp2.lower().decode('utf-8')))\n",
    "\n",
    "\n",
    "    for test in test_set_2:\n",
    "        temp = \"null \" + test[0]\n",
    "        temp2 = test[1] + \".\"\n",
    "        german_token_test.append(tokenizer(temp.lower().decode('utf-8')))\n",
    "        spanish_token_test_2.append(tokenizer(temp2.lower().decode('utf-8')))\n",
    "\n",
    "\n",
    "    for test in test_set_3:\n",
    "        temp = \"null \" + test[0]\n",
    "        temp2 = test[1] + \".\"\n",
    "        french_token_test.append(tokenizer(temp.lower().decode('utf-8')))\n",
    "        spanish_token_test_3.append(tokenizer(temp2.lower().decode('utf-8')))\n",
    "        \n",
    "        \n",
    "    language_test_pairs = np.column_stack((np.array(english_token_test), np.array(spanish_token_test)))\n",
    "    language_test_pairs_2 = np.column_stack((np.array(german_token_test), np.array(spanish_token_test_2)))\n",
    "    language_test_pairs_3 = np.column_stack((np.array(french_token_test), np.array(spanish_token_test_3)))\n",
    "    \n",
    "    \n",
    "    return None\n",
    "    \n",
    "    \n",
    "    \n",
    "def tokenize():\n",
    "    \n",
    "    global english_token_train\n",
    "    global spanish_token_train\n",
    "    global german_token_train\n",
    "    global spanish_token_train_2\n",
    "    global french_token_train\n",
    "    global spanish_token_train_3\n",
    "    global spanish_token_test_2\n",
    "    global spanish_token_test_3\n",
    "    global language_train_pairs\n",
    "    global language_train_pairs_2\n",
    "    global language_train_pairs_3\n",
    "    \n",
    "    english_token_train = []\n",
    "    spanish_token_train = []\n",
    "    german_token_train = []\n",
    "    spanish_token_train_2 = []\n",
    "    french_token_train = []\n",
    "    spanish_token_train_3 = []\n",
    "    \n",
    "    for train in training_set:\n",
    "        temp = \"null \" + train[0]\n",
    "        temp2 = train[1] + \".\"\n",
    "        english_token_train.append(tokenizer(temp.lower().decode('utf-8')))\n",
    "        spanish_token_train.append(tokenizer(temp2.lower().decode('utf-8')))\n",
    "\n",
    "\n",
    "    for train in training_set_2:\n",
    "        temp = \"null \" + train[0]\n",
    "        temp2 = train[1] + \".\"\n",
    "        german_token_train.append(tokenizer(temp.lower().decode('utf-8')))\n",
    "        spanish_token_train_2.append(tokenizer(temp2.lower().decode('utf-8')))\n",
    "\n",
    "\n",
    "    for train in training_set_3:\n",
    "        temp = \"null \" + train[0]\n",
    "        temp2 = train[1] + \".\"\n",
    "        french_token_train.append(tokenizer(temp.lower().decode('utf-8')))\n",
    "        spanish_token_train_3.append(tokenizer(temp2.lower().decode('utf-8')))\n",
    "\n",
    "\n",
    "    language_train_pairs = np.column_stack((np.array(english_token_train), np.array(spanish_token_train)))\n",
    "    language_train_pairs_2 = np.column_stack((np.array(german_token_train), np.array(spanish_token_train_2)))\n",
    "    language_train_pairs_3 = np.column_stack((np.array(french_token_train), np.array(spanish_token_train_3)))\n",
    "    \n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "english_types = None\n",
    "german_types  = None\n",
    "french_types  = None\n",
    "spanish_types = None\n",
    "\n",
    "def get_types():\n",
    "    global english_types\n",
    "    global german_types\n",
    "    global french_types\n",
    "    global spanish_types\n",
    "    \n",
    "    english_types = get_types_list(english_token_train)\n",
    "    german_types  = get_types_list(german_token_train)\n",
    "    french_types  = get_types_list(french_token_train)\n",
    "    spanish_types = get_types_list(spanish_token_train)\n",
    "\n",
    "    print \"\\nNumber of English types = \", len(english_types)\n",
    "    print \"Number of German types  = \", len(german_types)\n",
    "    print \"Number of French types  = \", len(french_types)\n",
    "    print \"Number of Spanish types = \", len(spanish_types)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def EM_translation_probabilities(formatted_language_pairs_corpus):\n",
    "    origin_language_types = get_types_list(column(formatted_language_pairs_corpus, 0))\n",
    "    target_language_types = get_types_list(column(formatted_language_pairs_corpus, 1))\n",
    "    \n",
    "    parameters_t = {}\n",
    "    shape_count_parameters_t = {}\n",
    "    \n",
    "    for origin_type in origin_language_types:\n",
    "        parameters_t[origin_type] = {}\n",
    "        shape_count_parameters_t[origin_type] = {}\n",
    "        \n",
    "    for entry in formatted_language_pairs_corpus:\n",
    "        for origin_word in entry[0]:\n",
    "            for target_word in entry[1]:\n",
    "                parameters_t[origin_word][target_word] = pow(10, -16)#np.random.uniform(pow(10, -16), pow(10, -10))\n",
    "                shape_count_parameters_t[origin_word][target_word] = 0.0\n",
    "    \n",
    "    i = 20\n",
    "    \n",
    "    while (i > 0.001):#JUST 3 DECIMAL PLACES SEEMS TO BE FINE\n",
    "        \n",
    "        #Expectation Step\n",
    "        counts = {}\n",
    "        for origin_word in shape_count_parameters_t:\n",
    "            counts[origin_word] = {}\n",
    "            for target_word in shape_count_parameters_t[origin_word]:\n",
    "                counts[origin_word][target_word] = 0\n",
    "                \n",
    "        total_origin = {}\n",
    "        for origin_type in origin_language_types:\n",
    "            total_origin[origin_type] = 0.0\n",
    "        \n",
    "        for entry in formatted_language_pairs_corpus:\n",
    "            target_word_total = {}\n",
    "            for target_word in entry[1]:\n",
    "                target_word_total[target_word] = 0.0\n",
    "                for origin_word in entry[0]:\n",
    "                    target_word_total[target_word] += parameters_t[origin_word][target_word]\n",
    "        \n",
    "            for target_word in entry[1]:\n",
    "                for origin_word in entry[0]:\n",
    "                    counts[origin_word][target_word] += parameters_t[origin_word][target_word]/target_word_total[target_word]\n",
    "                    total_origin[origin_word] += parameters_t[origin_word][target_word]/target_word_total[target_word]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Maximization Step\n",
    "        i = 0\n",
    "        parameters_t_new = {}    \n",
    "        for origin_word in shape_count_parameters_t:\n",
    "            parameters_t_new[origin_word] = {}\n",
    "            for target_word in shape_count_parameters_t[origin_word]:\n",
    "                parameters_t_new[origin_word][target_word] = counts[origin_word][target_word]/total_origin[origin_word]\n",
    "                temp = abs(parameters_t_new[origin_word][target_word] - parameters_t[origin_word][target_word])\n",
    "                if (i < temp):\n",
    "                    i = temp\n",
    "\n",
    "                    \n",
    "        parameters_t = parameters_t_new\n",
    "                    \n",
    "                   \n",
    "    ordered_parameters_t = {}\n",
    "    \n",
    "    for origin_type in origin_language_types:\n",
    "        parameters_t_for_index = []\n",
    "        word_for_index = []\n",
    "        word_for_value = []\n",
    "        i = 0\n",
    "        for key in shape_count_parameters_t[origin_type]:\n",
    "            word_for_index.append(key)\n",
    "            parameters_t_for_index.append([i, parameters_t_new[origin_type][key]])\n",
    "            i += 1\n",
    "        parameters_t_for_index = np.array(parameters_t_for_index)\n",
    "        ordered_parameters_t[origin_type] = parameters_t_for_index[parameters_t_for_index[:,1].argsort()[::-1]] #Nuno'S stolen magic\n",
    "        \n",
    "        \n",
    "        for i in range(len(ordered_parameters_t[origin_type])):\n",
    "            word_for_value.append([word_for_index[int(ordered_parameters_t[origin_type][i][0])], ordered_parameters_t[origin_type][i][1]])\n",
    "        ordered_parameters_t[origin_type] = word_for_value\n",
    "    \n",
    "    return ordered_parameters_t, parameters_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for element in ordered_parameters_t_IBM1_eng[\"to\"]:\n",
    "#    print element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def calculate_delta(parameters_t, parameters_q, source_sentence, target_sentence):\n",
    "    length_source = len(source_sentence)\n",
    "    length_target = len(target_sentence)\n",
    "    \n",
    "    delta_matrix = np.zeros((length_source, length_target))\n",
    "    normalizer = np.zeros(length_source)\n",
    "    \n",
    "    for i, source_word in enumerate(source_sentence):\n",
    "        for j, target_word in enumerate(target_sentence):\n",
    "            t = parameters_t[source_word][target_word]\n",
    "            q = parameters_q[length_source][length_target][i][j]\n",
    "            t_q = t*q\n",
    "            normalizer[i] += t_q\n",
    "            delta_matrix[i][j] = t_q\n",
    "            \n",
    "    return delta_matrix / normalizer[:, None]\n",
    "\n",
    "def EM_translation_probabilities_IBM2(formatted_language_pairs_corpus):\n",
    "    origin_language_types = get_types_list(column(formatted_language_pairs_corpus, 0))\n",
    "    target_language_types = get_types_list(column(formatted_language_pairs_corpus, 1))\n",
    "    \n",
    "    \n",
    "    parameters_t = {}\n",
    "    parameters_q = {}\n",
    "    \n",
    "    for origin_type in origin_language_types:\n",
    "        parameters_t[origin_type] = {}\n",
    "  \n",
    "    for entry in formatted_language_pairs_corpus:\n",
    "        for origin_word in entry[0]:\n",
    "            for target_word in entry[1]:\n",
    "                parameters_t[origin_word][target_word] = pow(10, -16)\n",
    "        \n",
    "        if len(entry[0]) not in parameters_q:\n",
    "            parameters_q[len(entry[0])] = {}\n",
    "        if len(entry[1]) not in parameters_q[len(entry[0])]:\n",
    "            parameters_q[len(entry[0])][len(entry[1])] = np.full((len(entry[0]), len(entry[1])), pow(10, -16))\n",
    "\n",
    "    x = 20\n",
    "    z = 20\n",
    "    \n",
    "    while (x > 0.01 or z > 0.01):#JUST 2 DECIMAL PLACES SEEMS TO BE FINE, otherwise takes too long\n",
    "        \n",
    "        #Set counts to 0\n",
    "        counts = {}\n",
    "        total_origin = {}\n",
    "        for origin_word in parameters_t:\n",
    "            counts[origin_word] = {}\n",
    "            total_origin[origin_word] = 0.0\n",
    "            for target_word in parameters_t[origin_word]:\n",
    "                counts[origin_word][target_word] = 0.0\n",
    "            \n",
    "            \n",
    "        q_counts = {}\n",
    "        q_counts_target = {}\n",
    "        for key_origin in parameters_q:\n",
    "            q_counts[key_origin] = {}\n",
    "            q_counts_target[key_origin] = {}\n",
    "            for key_target in parameters_q[key_origin]:\n",
    "                q_counts[key_origin][key_target] = np.zeros((key_origin, key_target))\n",
    "                q_counts_target[key_origin][key_target] = np.zeros(key_target)\n",
    "        \n",
    "        \n",
    "        #Expectation Step\n",
    "        for entry in formatted_language_pairs_corpus:\n",
    "            delta = calculate_delta(parameters_t, parameters_q, entry[0], entry[1])\n",
    "            source_length = len(entry[0])\n",
    "            target_length = len(entry[1])\n",
    "            for i, origin_word in enumerate(entry[0]):\n",
    "                for j, target_word in enumerate(entry[1]):\n",
    "                    val = delta[i][j]\n",
    "                    counts[origin_word][target_word] += val\n",
    "                    total_origin[origin_word] += val\n",
    "                    q_counts[source_length][target_length][i][j] += val\n",
    "                    q_counts_target[source_length][target_length][j] += val\n",
    "        \n",
    "        \n",
    "        #Maximization Step\n",
    "        x = 0\n",
    "        parameters_t_new = {}    \n",
    "        for origin_word in parameters_t:\n",
    "            parameters_t_new[origin_word] = {}\n",
    "            c_o = total_origin[origin_word]\n",
    "            for target_word in parameters_t[origin_word]:\n",
    "                c   = counts[origin_word][target_word]\n",
    "                parameters_t_new[origin_word][target_word] = c/c_o\n",
    "                temp = abs(parameters_t_new[origin_word][target_word] - parameters_t[origin_word][target_word])\n",
    "                if (x < temp):\n",
    "                    x = temp\n",
    "                    \n",
    "        parameters_t = parameters_t_new\n",
    "                \n",
    "            \n",
    "        parameters_q_new = {}\n",
    "        z = 0\n",
    "        for key_origin in parameters_q:\n",
    "            parameters_q_new[key_origin] = {}\n",
    "            for key_target in parameters_q[key_origin]:\n",
    "                mat = q_counts[key_origin][key_target] / q_counts_target[key_origin][key_target][None, :]\n",
    "                parameters_q_new[key_origin][key_target] = mat\n",
    "                temp = abs((parameters_q_new[key_origin][key_target] - parameters_q[key_origin][key_target]).max())\n",
    "                if (z < temp):\n",
    "                    z = temp\n",
    "    \n",
    "        parameters_q = parameters_q_new\n",
    "                   \n",
    "    ordered_parameters_t = {}\n",
    "    \n",
    "    for origin_type in origin_language_types:\n",
    "        parameters_t_for_index = []\n",
    "        word_for_index = []\n",
    "        word_for_value = []\n",
    "        i = 0\n",
    "        for key in parameters_t_new[origin_type]:\n",
    "            word_for_index.append(key)\n",
    "            parameters_t_for_index.append([i, parameters_t_new[origin_type][key]])\n",
    "            i += 1\n",
    "        parameters_t_for_index = np.array(parameters_t_for_index)\n",
    "        ordered_parameters_t[origin_type] = parameters_t_for_index[parameters_t_for_index[:,1].argsort()[::-1]] #Nuno'S stolen magic\n",
    "        \n",
    "        \n",
    "        for i in range(len(ordered_parameters_t[origin_type])):\n",
    "            word_for_value.append([word_for_index[int(ordered_parameters_t[origin_type][i][0])], ordered_parameters_t[origin_type][i][1]])\n",
    "        ordered_parameters_t[origin_type] = word_for_value\n",
    "    \n",
    "    return ordered_parameters_t, parameters_t, parameters_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for element in ordered_parameters_t_IBM2_eng[\"from\"]:\n",
    "#    print element"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# dump dictionary to file. the result file is not human readable\n",
    "import pickle\n",
    "with open('ordered_prob_eng_ind.txt', 'wb') as handle:\n",
    "    pickle.dump(ordered_parameters_t, handle)\n",
    "    \n",
    "with open('parameters_t_eng_ind.txt', 'wb') as handle:\n",
    "    pickle.dump(parameters_t, handle)\n",
    "    \n",
    "#with open('dictionary_eng_ind.txt', 'rb') as handle:\n",
    "#    translation_probabilities = pickle.loads(handle.read())\n",
    "    \n",
    "#with open('result.txt', 'rb') as handle:\n",
    "#    ordered_key_probabilities = pickle.loads(handle.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'origin1 = language_test_pairs[0][0]\\ntarget1 = language_test_pairs[0][1]\\nresult1 = IBM1(origin1, target1, parameters_t_IBM1_eng)'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def IBM1(origin_sentence, target_sentence, parameters_t):\n",
    "    \n",
    "    best_alignment = [0]*len(target_sentence)\n",
    "    for i,target_word in enumerate(target_sentence):\n",
    "        max = 0\n",
    "        for j, source_word in enumerate(origin_sentence):\n",
    "            if source_word in parameters_t and target_word in parameters_t[source_word]:\n",
    "                \n",
    "                if max < parameters_t[source_word][target_word]:\n",
    "                    best_alignment[i] = j\n",
    "                    max = parameters_t[source_word][target_word]\n",
    "                    \n",
    "    return best_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'origin2 = language_test_pairs[0][0]\\ntarget2 = language_test_pairs[0][1]\\nresult2 = IBM2(origin2, target2, parameters_t_IBM2_eng, parameters_q_IBM2_eng)'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def IBM2(origin_sentence, target_sentence, parameters_t, parameters_q):\n",
    "    \n",
    "    _DEBUG = True\n",
    "    \n",
    "    best_alignment = [0]*len(target_sentence)\n",
    "    for i, target_word in enumerate(target_sentence):\n",
    "        max = 0\n",
    "        \n",
    "        if _DEBUG:\n",
    "            print \"NEW TARGET WORD PICKED: \", target_word\n",
    "            \n",
    "        for j, source_word in enumerate(origin_sentence):\n",
    "            if source_word in parameters_t and target_word in parameters_t[source_word]:\n",
    "                if(len(origin_sentence) not in parameters_q):\n",
    "                    q = 1\n",
    "                    if _DEBUG:\n",
    "                        print \"UNKNOWN SENTENCE LENGTH\"\n",
    "                else:\n",
    "                    if(len(target_sentence) not in parameters_q[len(origin_sentence)]):\n",
    "                        q = 1\n",
    "                        if _DEBUG:\n",
    "                            print \"UNKNOWN SENTENCE LENGTH\"\n",
    "                    else:\n",
    "                        q = parameters_q[len(origin_sentence)][len(target_sentence)][j][i]\n",
    "                temp = q * parameters_t[source_word][target_word]\n",
    "                \n",
    "                if _DEBUG:\n",
    "                        print temp, q, parameters_t[source_word][target_word]\n",
    "                        \n",
    "                if max < temp :\n",
    "                    best_alignment[i] = j\n",
    "                    max = temp\n",
    "            else:\n",
    "                t = 1\n",
    "                if source_word not in parameters_t:\n",
    "                    if _DEBUG:\n",
    "                        print \"UNKNOWN SOURCE WORD: \", source_word\n",
    "                else:\n",
    "                    if target_word not in parameters_t[source_word]:\n",
    "                        if _DEBUG:\n",
    "                            print \"UNKNOWN TARGET WORD\", target_word\n",
    "\n",
    "                if(len(origin_sentence) not in parameters_q):\n",
    "                    q = 0\n",
    "                    if _DEBUG:\n",
    "                        print \"UNKNOWN SENTENCE LENGTH\"\n",
    "                else:\n",
    "                    if(len(target_sentence) not in parameters_q[len(origin_sentence)]):\n",
    "                        q = 0\n",
    "                        if _DEBUG:\n",
    "                            print \"UNKNOWN SENTENCE LENGTH\"\n",
    "                    else:\n",
    "                        q = parameters_q[len(origin_sentence)][len(target_sentence)][j][i]\n",
    "                        \n",
    "                temp = q * t\n",
    "            \n",
    "                if _DEBUG:\n",
    "                    print temp, q, t\n",
    "                    \n",
    "                if max < temp:\n",
    "                    best_alignment[i] = j\n",
    "                    max = temp\n",
    "                    \n",
    "    return best_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''origin1 = language_test_pairs[0][0]\n",
    "#print origin1\n",
    "target1 = language_test_pairs[0][1]\n",
    "#print target1\n",
    "result1 = IBM1(origin1, target1, parameters_t_IBM1_eng)\n",
    "\n",
    "print \"----------\"\n",
    "\n",
    "result_print = []\n",
    "for i in range(len(result1)):\n",
    "    result_print.append(str(target1[i].encode('utf-8') + \" \" + str(result1[i])))\n",
    "print result_print \n",
    "result_origin = []\n",
    "for i in range(len(origin1)):\n",
    "    result_origin.append(str(origin1[i].encode('utf-8') + \" \" + str(i)))\n",
    "print result_origin'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'result_print = []\\nfor i in range(len(result2)):\\n    result_print.append(str(target2[i].encode(\\'utf-8\\') + \" \" + str(result2[i])))\\nprint result_print \\nresult_origin = []\\nfor i in range(len(language_test_pairs[0][0])):\\n    result_origin.append(str(origin2[i].encode(\\'utf-8\\') + \" \" + str(i)))\\nprint result_origin'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''origin2 = language_test_pairs[0][0]\n",
    "#print origin2\n",
    "target2 = language_test_pairs[0][1]\n",
    "#print target2\n",
    "result2 = IBM2(origin2, target2, parameters_t_IBM2_eng, parameters_q_IBM2_eng)\n",
    "\n",
    "print \"----------\"\n",
    "\n",
    "result_print = []\n",
    "for i in range(len(result2)):\n",
    "    result_print.append(str(target2[i].encode('utf-8') + \" \" + str(result2[i])))\n",
    "print result_print \n",
    "result_origin = []\n",
    "for i in range(len(language_test_pairs[0][0])):\n",
    "    result_origin.append(str(origin2[i].encode('utf-8') + \" \" + str(i)))\n",
    "print result_origin'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "java_path = \"C:\\Program Files\\Java\\jdk1.8.0_111\"\n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pos_tagger_english(sentence):\n",
    "    path_to_model = \"stanford-postagger-full-2015-12-09/models/english-bidirectional-distsim.tagger\"\n",
    "    path_to_jar = \"stanford-postagger-full-2015-12-09/stanford-postagger.jar\"\n",
    "    tagger=StanfordPOSTagger(path_to_model, path_to_jar)\n",
    "    tagger.java_options='-mx1000m'\n",
    "    return tagger.tag(sentence)\n",
    "    \n",
    "def pos_tagger_german(sentence):\n",
    "    path_to_model = \"stanford-postagger-full-2015-12-09/models/german-hgc.tagger\"\n",
    "    path_to_jar = \"stanford-postagger-full-2015-12-09/stanford-postagger.jar\"\n",
    "    tagger=StanfordPOSTagger(path_to_model, path_to_jar)\n",
    "    tagger.java_options='-mx1000m'\n",
    "    return tagger.tag(sentence)\n",
    "\n",
    "def pos_tagger_french(sentence):\n",
    "    path_to_model = \"stanford-postagger-full-2015-12-09/models/french.tagger\"\n",
    "    path_to_jar = \"stanford-postagger-full-2015-12-09/stanford-postagger.jar\"\n",
    "    tagger=StanfordPOSTagger(path_to_model, path_to_jar)\n",
    "    tagger.java_options='-mx1000m'\n",
    "    return tagger.tag(sentence)\n",
    "\n",
    "def pos_tagger_spanish(sentence):\n",
    "    path_to_model = \"stanford-postagger-full-2015-12-09/models/spanish.tagger\"\n",
    "    path_to_jar = \"stanford-postagger-full-2015-12-09/stanford-postagger.jar\"\n",
    "    tagger=StanfordPOSTagger(path_to_model, path_to_jar)\n",
    "    tagger.java_options='-mx1000m'\n",
    "    return tagger.tag(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eng_dict = {}\n",
    "with open(\"postag-highsentence/english.map\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        res = line.split()\n",
    "        eng_dict[res[0]] = res[1]\n",
    "\n",
    "ger_dict = {}\n",
    "with open(\"postag-highsentence/german.map\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        res = line.split()\n",
    "        ger_dict[res[0]] = res[1]\n",
    "\n",
    "fre_dict = {}\n",
    "with open(\"postag-highsentence/french.map\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        res = line.split()\n",
    "        if(res[0] == \"PONCT\"):\n",
    "            fre_dict[\"PUNC\"] = res[1]\n",
    "        else:\n",
    "            fre_dict[res[0]] = res[1]\n",
    "            \n",
    "spa_dict = {}\n",
    "with open(\"postag-highsentence/spanish.map\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        res = line.split()\n",
    "        spa_dict[res[0]] = res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapper(sentences, universal_dict):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, word_tag in enumerate(sentence):\n",
    "            if(word_tag[1] in universal_dict):\n",
    "                temp = list(sentences[i][j])\n",
    "                temp[1] = universal_dict[word_tag[1]]\n",
    "                sentences[i][j] = tuple(temp)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_sentences_universal = None\n",
    "german_sentences_universal  = None\n",
    "french_sentences_universal  = None\n",
    "spanish_sentences_universal = None\n",
    "\n",
    "def tag_test_sentences():\n",
    "    \n",
    "    global english_sentences_universal\n",
    "    global german_sentences_universal\n",
    "    global french_sentences_universal\n",
    "    global spanish_sentences_universal\n",
    "\n",
    "    english_sentences = []\n",
    "    for sentence in language_test_pairs:\n",
    "        english_sentences.append(pos_tagger_english(sentence[0]))\n",
    "    english_sentences_universal = mapper(english_sentences, eng_dict)\n",
    "    \n",
    "    # german will take some time to tag\n",
    "    german_sentences = []\n",
    "    for sentence in language_test_pairs_2:\n",
    "        german_sentences.append(pos_tagger_german(sentence[0]))\n",
    "    german_sentences_universal = mapper(german_sentences, ger_dict)\n",
    "    \n",
    "    french_sentences = []\n",
    "    for sentence in language_test_pairs_3:\n",
    "        french_sentences.append(pos_tagger_french(sentence[0]))\n",
    "    french_sentences_universal = mapper(french_sentences, fre_dict)\n",
    "    \n",
    "    spanish_sentences = []\n",
    "    for sentence in language_test_pairs:\n",
    "        spanish_sentences.append(pos_tagger_spanish(sentence[1]))\n",
    "    spanish_sentences_universal = mapper(spanish_sentences, spa_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_sentences = None\n",
    "language_family = None\n",
    "\n",
    "def get_source_sentences_tagged():\n",
    "    \n",
    "    global source_sentences\n",
    "    global language_family\n",
    "    \n",
    "    source_sentences = []\n",
    "    for i in range(len(english_sentences_universal)):\n",
    "        source_sentences.append([[english_sentences_universal[i], german_sentences_universal[i],\n",
    "                                  french_sentences_universal[i]], language_test_pairs[i][1]])\n",
    "\n",
    "    english_language_families = [\"Indo-European\", \"Germanic\", \"West\"]\n",
    "    german_language_families  = [\"Indo-European\", \"Germanic\", \"West\"]\n",
    "    french_language_families  = [\"Indo-European\", \"Italic\", \"Romance\"]\n",
    "    spanish_language_families = [\"Indo-European\", \"Italic\", \"Romance\"]\n",
    "    language_family = [english_language_families, german_language_families, \n",
    "                       french_language_families, spanish_language_families]\n",
    "#print source_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def quorum(tags):\n",
    "    data = collections.Counter(tags)\n",
    "    return data.most_common(1)[0][0]\n",
    "\n",
    "def propagateTags(source_sentences, target_sentence, parameters_t_tables, \n",
    "                  parameters_q_tables, weighted, language_family):\n",
    "    alignments = []\n",
    "    tagged_target_sentence = []\n",
    "    for word in target_sentence:\n",
    "        tagged_target_sentence.append([word])\n",
    "    for i, sentences in enumerate(source_sentences):\n",
    "        untagged_sentence = []\n",
    "        for tagged_word in sentences:\n",
    "            untagged_sentence.append(tagged_word[0])\n",
    "        if parameters_q_tables is None:\n",
    "            alignments.append(IBM1(untagged_sentence, target_sentence, parameters_t_tables[i]))\n",
    "        else:\n",
    "            alignments.append(IBM2(untagged_sentence, target_sentence, parameters_t_tables[i], parameters_q_tables[i]))\n",
    "\n",
    "            \n",
    "    if not weighted:    \n",
    "        for i, word in enumerate(target_sentence):\n",
    "            tags = []\n",
    "            for j, alignment in enumerate(alignments):\n",
    "                tag = source_sentences[j][alignment[i]][1]\n",
    "                tags.append(tag)\n",
    "            tagged_target_sentence[i].append(quorum(tags))\n",
    "            for elem in alignments:\n",
    "                tagged_target_sentence[i].append(elem[i])\n",
    "                \n",
    "    else:\n",
    "        for i, word in enumerate(target_sentence):\n",
    "            tags = {}\n",
    "            for j, alignment in enumerate(alignments):\n",
    "                tag = source_sentences[j][alignment[i]][1]\n",
    "                \n",
    "                if(language_family[0][j][0] == language_family[0][-1][0]):\n",
    "                    if(language_family[0][j][1] == language_family[0][-1][1]):\n",
    "                        if(language_family[0][j][2] == language_family[0][-1][2]):\n",
    "                            val = 2.5\n",
    "                        else:\n",
    "                            val = 2\n",
    "                    else:\n",
    "                        val = 1.5\n",
    "                else:\n",
    "                    val = 1\n",
    "                    \n",
    "                if tag in tags:\n",
    "                    tags[tag] += val\n",
    "                else:\n",
    "                    tags[tag] = val\n",
    "            \n",
    "            tags_l = []\n",
    "            for tag in tags:\n",
    "                tags[tag] = int(tags[tag])\n",
    "                while(tags[tag] > 0):\n",
    "                    tags_l.append(tag)\n",
    "                    tags[tag] -= 1\n",
    "            \n",
    "            tagged_target_sentence[i].append(quorum(tags_l))\n",
    "            for elem in alignments:\n",
    "                tagged_target_sentence[i].append(elem[i])\n",
    "    \n",
    "    \n",
    "    return tagged_target_sentence\n",
    "\n",
    "\n",
    "def tag_low_resource_sentences(sentence_groups, parameters_t_tables, parameters_q_tables,\n",
    "                               weighted, language_family, unique_language_index):\n",
    "    tagged_target_sentences = []\n",
    "    \n",
    "    if unique_language_index is None:\n",
    "        for entry in sentence_groups:\n",
    "            tagged_target_sentences.append(propagateTags(entry[0], entry[1], parameters_t_tables,\n",
    "                                                         parameters_q_tables, weighted, language_family))\n",
    "    else:\n",
    "        for entry in sentence_groups:\n",
    "            tagged_target_sentences.append(propagateTags([entry[0][unique_language_index]], entry[1],\n",
    "                                                         parameters_t_tables, parameters_q_tables, \n",
    "                                                         weighted, language_family))\n",
    "\n",
    "    return tagged_target_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print source_sentences[0]\\nprint \"-----------\"\\nprint spanish_sentences_universal[0]\\nprint \"-----------\"\\ncalculate_accuracy([source_sentences[0]], spanish_sentences_universal, 0)'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_accuracy(source_sentences, spanish_sentences_universal):\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for l in range(len(source_sentences[0][0])):\n",
    "        temp_result = tag_low_resource_sentences(source_sentences, parameters_t_all_IBM1,\n",
    "                                             None, False, None, l)\n",
    "        \n",
    "        i = 0.0\n",
    "        total_words = 0.0\n",
    "        for j in range(len(temp_result)):\n",
    "            for k in range(len(temp_result[j])):\n",
    "                if temp_result[j][k][1] == spanish_sentences_universal[j][k][1]:\n",
    "                    i += 1\n",
    "            total_words += k\n",
    "\n",
    "        language_l = i / total_words\n",
    "        accuracies.append(language_l)\n",
    "    \n",
    "    \n",
    "    result_IBM1 = tag_low_resource_sentences(source_sentences, parameters_t_all_IBM1,\n",
    "                                             None, False, None, None)\n",
    "    i = 0.0\n",
    "    total_words = 0.0\n",
    "    for j in range(len(result_IBM1)):\n",
    "        for k in range(len(result_IBM1[j])):\n",
    "            if result_IBM1[j][k][1] == spanish_sentences_universal[j][k][1]:\n",
    "                i += 1\n",
    "        total_words += k\n",
    "    \n",
    "    IBM1 = i / total_words\n",
    "    accuracies.append(IBM1)\n",
    "    #print \"IBM1: \", IBM1\n",
    "    \n",
    "    \n",
    "    \n",
    "    result_IBM1 = tag_low_resource_sentences(source_sentences, parameters_t_all_IBM1,\n",
    "                                             None, True, language_family, None)\n",
    "    i = 0.0\n",
    "    total_words = 0.0\n",
    "    for j in range(len(result_IBM1)):\n",
    "        for k in range(len(result_IBM1[j])):\n",
    "            if result_IBM1[j][k][1] == spanish_sentences_universal[j][k][1]:\n",
    "                i += 1\n",
    "        total_words += k\n",
    "    \n",
    "    IBM1_WEIGHTED = i / total_words\n",
    "    accuracies.append(IBM1_WEIGHTED)\n",
    "    #print \"IBM1 WEIGHTED: \", IBM1_WEIGHTED\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''for l in range(len(source_sentences[0][0])):\n",
    "        temp_result = tag_low_resource_sentences(source_sentences, parameters_t_all_IBM2,\n",
    "                                             parameters_q_all_IBM2, False, None, l)\n",
    "        \n",
    "        i = 0.0\n",
    "        total_words = 0.0\n",
    "        for j in range(len(temp_result)):\n",
    "            for k in range(len(temp_result[j])):\n",
    "                if temp_result[j][k][1] == spanish_sentences_universal[j][k][1]:\n",
    "                    i += 1\n",
    "            total_words += k\n",
    "\n",
    "        language_l = i / total_words\n",
    "        accuracies.append(language_l)\n",
    "    \n",
    "    \n",
    "    result_IBM2 = tag_low_resource_sentences(source_sentences, parameters_t_all_IBM2,\n",
    "                                             parameters_q_all_IBM2, False, None, None)\n",
    "    i = 0.0\n",
    "    total_words = 0.0\n",
    "    for j in range(len(result_IBM2)):\n",
    "        for k in range(len(result_IBM2[j])):\n",
    "            if result_IBM2[j][k][1] == spanish_sentences_universal[j][k][1]:\n",
    "                i += 1\n",
    "        total_words += k\n",
    "        \n",
    "    IBM2 = i / total_words\n",
    "    accuracies.append(IBM2)\n",
    "    #print \"IBM2: \", IBM2\n",
    "    \n",
    "    \n",
    "    result_IBM2 = tag_low_resource_sentences(source_sentences, parameters_t_all_IBM2,\n",
    "                                             parameters_q_all_IBM2, True, language_family, None)\n",
    "    i = 0.0\n",
    "    total_words = 0.0\n",
    "    for j in range(len(result_IBM2)):\n",
    "        for k in range(len(result_IBM2[j])):\n",
    "            if result_IBM2[j][k][1] == spanish_sentences_universal[j][k][1]:\n",
    "                i += 1\n",
    "        total_words += k\n",
    "        \n",
    "    IBM2_WEIGHTED = i / total_words\n",
    "    accuracies.append(IBM2_WEIGHTED)\n",
    "    #print \"IBM2_WEIGHTED: \", IBM2_WEIGHTED'''\n",
    "    \n",
    "    \n",
    "    print accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_alignment(result):\n",
    "    result_print = []\n",
    "    for i in range(len(result)):\n",
    "        result_print.append(str(target[i] + \" \" + str(result[i])))\n",
    "    print result_print \n",
    "    result_origin = []\n",
    "    for i in range(len(origin)):\n",
    "        result_origin.append(str(origin[i].encode('utf-8'))+ \" \" + str(i))\n",
    "    print result_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pick_test_sentences(100)\\nsave_test_set()\\ntokenize_test()\\ntag_test_sentences()\\nget_source_sentences_tagged()'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pick_test_sentences(100)\n",
    "save_test_set()\n",
    "tokenize_test()\n",
    "tag_test_sentences()\n",
    "get_source_sentences_tagged()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'number_of_sentences = [10, 100, 1000, 5000, 10000, 15000, 20000, 25000, 30000]\\nfor i, number in enumerate(number_of_sentences):\\n    pick_training_sentences(number)\\n    save_training_set(number)'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''number_of_sentences = [10, 100, 1000, 5000, 10000, 15000, 20000, 25000, 30000]\n",
    "for i, number in enumerate(number_of_sentences):\n",
    "    pick_training_sentences(number)\n",
    "    save_training_set(number)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_data(number):\n",
    "    global training_set\n",
    "    global training_set_2\n",
    "    global training_set_3\n",
    "    with open(\"experiment_data/english_\" + str(number) + \".txt\", 'rb') as handle:\n",
    "        training_set = pickle.loads(handle.read())\n",
    "    with open(\"experiment_data/german_\" + str(number) + \".txt\", 'rb') as handle:\n",
    "        training_set_2 = pickle.loads(handle.read())\n",
    "    with open(\"experiment_data/french_\" + str(number) + \".txt\", 'rb') as handle:\n",
    "        training_set_3 = pickle.loads(handle.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF SENTENCES =  10000\n",
      "\n",
      "Number of English types =  8423\n",
      "Number of German types  =  12901\n",
      "Number of French types  =  15872\n",
      "Number of Spanish types =  16672\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-3f3c7a9459d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-80-3f3c7a9459d7>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mordered_parameters_t_IBM2_ger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_t_IBM2_ger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_q_IBM2_ger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEM_translation_probabilities_IBM2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_train_pairs_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;31m#print \"FRENCH\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mordered_parameters_t_IBM2_fre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_t_IBM2_fre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_q_IBM2_fre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEM_translation_probabilities_IBM2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_train_pairs_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-7b2ebd466c5f>\u001b[0m in \u001b[0;36mEM_translation_probabilities_IBM2\u001b[0;34m(formatted_language_pairs_corpus)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m#Expectation Step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformatted_language_pairs_corpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_delta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0msource_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mtarget_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-7b2ebd466c5f>\u001b[0m in \u001b[0;36mcalculate_delta\u001b[0;34m(parameters_t, parameters_q, source_sentence, target_sentence)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters_q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlength_source\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlength_target\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mt_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mnormalizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt_q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters_t_all_IBM1 = None\n",
    "parameters_t_all_IBM2 = None\n",
    "parameters_q_all_IBM2 = None\n",
    "\n",
    "number_of_sentences = [10, 100, 1000, 5000, 10000, 15000, 20000, 25000, 30000]\n",
    "number_of_trials = 3\n",
    "\n",
    "def get_data():\n",
    "    \n",
    "    global parameters_t_all_IBM1\n",
    "    global parameters_t_all_IBM2\n",
    "    global parameters_q_all_IBM2\n",
    "    \n",
    "    import time\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for i in range(3):\n",
    "        results.append([])#IBM1(0), IBM2(1), Fast-align(2)\n",
    "        for j in range(len(source_sentences[0][0]) + 2):\n",
    "            results[i].append([])#Directly from Eng(0), Ger(1) and Fre(2), with quorum(3) and with weighted quorum(4)\n",
    "            results[i][j] = np.zeros(len(number_of_sentences))\n",
    "    \n",
    "    \n",
    "    for i, number in enumerate(number_of_sentences):\n",
    "        for k in range(number_of_trials):\n",
    "\n",
    "            if (number != 10000 or k ==2):\n",
    "                if(k == 0 and i != 0):\n",
    "                    print \"\\n\\n\\n\\n\\n\"\n",
    "                print \"NUMBER OF SENTENCES = \", number\n",
    "\n",
    "                load_train_data(number)\n",
    "                tokenize()\n",
    "                get_types()\n",
    "\n",
    "\n",
    "                import time\n",
    "                start_time = time.time()\n",
    "\n",
    "                ordered_parameters_t_IBM1_eng, parameters_t_IBM1_eng = EM_translation_probabilities(language_train_pairs)\n",
    "                ordered_parameters_t_IBM1_ger, parameters_t_IBM1_ger = EM_translation_probabilities(language_train_pairs_2)\n",
    "                ordered_parameters_t_IBM1_fre, parameters_t_IBM1_fre = EM_translation_probabilities(language_train_pairs_3)\n",
    "\n",
    "                time = time.time() - start_time\n",
    "                print \"\\n\\nFINISHED ESTIMATING PARAMETERS_IBM1\"\n",
    "                print(\"--- %s seconds ---\" % (time))\n",
    "\n",
    "                parameters_t_all_IBM1 = []\n",
    "                parameters_t_all_IBM1.append(parameters_t_IBM1_eng)\n",
    "                parameters_t_all_IBM1.append(parameters_t_IBM1_ger)\n",
    "                parameters_t_all_IBM1.append(parameters_t_IBM1_fre)\n",
    "\n",
    "                dump_model(\"parameters_t_eng_IBM1_\" + str(number) + \"_\" + str(k) + \".txt\", parameters_t_IBM1_eng)\n",
    "                dump_model(\"parameters_t_ger_IBM1_\" + str(number) + \"_\" + str(k) + \".txt\", parameters_t_IBM1_ger)\n",
    "                dump_model(\"parameters_t_fre_IBM1_\" + str(number) + \"_\" + str(k) + \".txt\", parameters_t_IBM1_fre)\n",
    "\n",
    "\n",
    "\n",
    "                '''import time\n",
    "                start_time = time.time()\n",
    "                #print \"ENGLISH\"\n",
    "                ordered_parameters_t_IBM2_eng, parameters_t_IBM2_eng, parameters_q_IBM2_eng = EM_translation_probabilities_IBM2(language_train_pairs)\n",
    "                #print \"GERMAN\"\n",
    "                ordered_parameters_t_IBM2_ger, parameters_t_IBM2_ger, parameters_q_IBM2_ger = EM_translation_probabilities_IBM2(language_train_pairs_2)\n",
    "                #print \"FRENCH\"\n",
    "                ordered_parameters_t_IBM2_fre, parameters_t_IBM2_fre, parameters_q_IBM2_fre = EM_translation_probabilities_IBM2(language_train_pairs_3)\n",
    "\n",
    "                time = time.time() - start_time\n",
    "                print \"\\nFINISHED ESTIMATING PARAMETERS_IBM2\"\n",
    "                print(\"--- %s seconds ---\" % (time))\n",
    "\n",
    "\n",
    "                parameters_t_all_IBM2 = []\n",
    "                parameters_q_all_IBM2 = []\n",
    "\n",
    "                parameters_t_all_IBM2.append(parameters_t_IBM2_eng)\n",
    "                parameters_t_all_IBM2.append(parameters_t_IBM2_eng)\n",
    "                parameters_t_all_IBM2.append(parameters_t_IBM2_eng)\n",
    "\n",
    "                parameters_q_all_IBM2.append(parameters_q_IBM2_eng)\n",
    "                parameters_q_all_IBM2.append(parameters_q_IBM2_eng)\n",
    "                parameters_q_all_IBM2.append(parameters_q_IBM2_eng)\n",
    "\n",
    "\n",
    "                dump_model(\"parameters_t_eng_IBM2_\" + str(number) + \"_\" + str(k) + \".txt\", parameters_t_IBM2_eng)\n",
    "                dump_model(\"parameters_t_ger_IBM2_\" + str(number) + \"_\" + str(k) + \".txt\", parameters_t_IBM2_ger)\n",
    "                dump_model(\"parameters_t_fre_IBM2_\" + str(number) + \"_\" + str(k) + \".txt\", parameters_t_IBM2_fre)     \n",
    "\n",
    "                dump_model(\"parameters_q_eng_IBM2_\" + str(number) + \"_\" + str(k) + \".txt\", parameters_q_IBM2_eng)\n",
    "                dump_model(\"parameters_q_ger_IBM2_\" + str(number) + \"_\" + str(k) + \".txt\", parameters_q_IBM2_ger)\n",
    "                dump_model(\"parameters_q_fre_IBM2_\" + str(number) + \"_\" + str(k) + \".txt\", parameters_q_IBM2_fre)'''\n",
    "\n",
    "            \n",
    "            accuracies = calculate_accuracy(source_sentences, spanish_sentences_universal)\n",
    "            for j in range(len(results[0])):\n",
    "                results[0][j][i] += accuracies[j]\n",
    "        \n",
    "        \n",
    "    for j in range(len(results[0])):\n",
    "        results[0][j] /= number_of_trials\n",
    "        \n",
    "    print number_of_sentences\n",
    "    print results[0]\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.plot(number_of_sentences, results[0][0], 'cs',\n",
    "             number_of_sentences, results[0][1], 'bs',\n",
    "             number_of_sentences, results[0][2], 'ks',\n",
    "             number_of_sentences, results[0][3], 'gs',\n",
    "             number_of_sentences, results[0][4], 'rs')\n",
    "    \n",
    "    import time\n",
    "    time = time.time() - start_total_time\n",
    "    print \"\\nFINISHED GETTING DATA\"\n",
    "    print(\"--- %s seconds ---\" % (time))\n",
    "        \n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
